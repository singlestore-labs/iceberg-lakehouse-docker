# Fast Deployment of Iceberg Lakehouse with Multiple Catalogs

## Overview

This repository deploys an **Iceberg lakehouse** with multiple catalogs using **Docker Compose**.  
It's based on [apache/iceberg-python](https://github.com/apache/iceberg-python) with modifications to support more catalogs and advanced Hive security.

This setup includes:
- A **query engine** (Spark)
- A **data storage layer** (MinIO)
- Multiple **catalogs** (Rest, Hive, Jdbc, Hadoop)

A **provisioning Python script** (`catalog_provision.py`) writes a simple Iceberg table into each catalog.  
After deployment, you can query Iceberg tables using either [SingleStore](https://www.singlestore.com/) or [Spark](https://spark.apache.org/).

## Deployment

```sh
docker compose -f docker-compose-integration.yml kill
docker compose -f docker-compose-integration.yml rm -f
docker compose -f docker-compose-integration.yml up -d
sleep 10
docker compose -f docker-compose-integration.yml exec -T spark-iceberg python catalog_provision.py
```

## Explore the data

```
docker compose -f docker-compose-integration.yml exec -it spark-iceberg spark-sql
spark-sql ()> select * from rest.default.test_table;
1
2
3
4
5
```
Similarly, you can use Spark with the Hadoop/Jdbc catalog.

As for the Hive catalog, the latest Spark isn't compatible with the Hive security used, so a different engine is needed.
I use SingleStore database to read data from a Hive Catalog.
```
docker compose -f docker-compose-singlestore.yml up
docker compose -f docker-compose-singlestore.yml exec -it singlsestoredb-dev singlestore -proot
```

Create a table and use [SingleStore docs](https://docs.singlestore.com/db/v8.9/load-data/data-sources/iceberg-ingest/) to setup an Iceberg pipeline.
```
create databasete test_db; use test_db; create table t (col int);
CREATE PIPELINE p AS LOAD DATA s3 'default.test_table' CONFIG '{"catalog_type": "HIVE", "catalog.uri": "http://hive:9083", "endpoint_url": "http://minio:9000", "region": "us-east1", "catalog.hive.metastore.client.auth.mode": "PLAIN", "catalog.hive.metastore.client.plain.username": "hmsuser", "catalog.hive.metastore.client.plain.password": "hmspasswd", "catalog.metastore.use.SSL": "true", "catalog.hive.metastore.truststore.type": "PKCS12", "catalog.hive.metastore.truststore.path": "/tmp/hive-truststore.p12", "catalog.hive.metastore.truststore.password": "changeit"}' CREDENTIALS '{"aws_access_key_id": "admin", "aws_secret_access_key": "password"}' INTO TABLE t format ICEBERG (col <- %::col);

singlestore> test pipeline p;
+------+
| col  |
+------+
|    1 |
|    2 |
|    3 |
|    4 |
|    5 |
+------+
5 rows in set
```

## Services

- **Spark-Iceberg**: Handles Spark and Iceberg integration by executing `catalog_provision.py` to write one table into all catalogs.
- **MinIO**: Provides an **S3-compatible** object store for Iceberg tables.
- **Hive Metastore**: Configured with **enhanced security settings** â€“ SSL and password-based authentication.
- **REST Catalog**: `tabulario/iceberg-rest` container that can be any implementation under the hood.
- **MinIO Client (`mc`)**: Used for **bucket provisioning**.
- **JDBC Catalog**: Uses **SQLite JDBC** to a file for simplicity.
- **Hadoop Catalog**: Uses **local filesystem** under `/tmp/` outside the Docker image for external reader access.
- **SingleSore cluster**: [SingleStore](https://www.singlestore.com/) **cluster** deployment inside a docker as an alternative reader to Spark.

## Notes

- Latest **Spark** **does not support Hive SSL & password authentication**, so a **Java client** is used to provision a table (`HiveProvision.java`) and SingleStore is used to read it.
- **Certificate generation instructions** are available in `utils/hive_certs_instruction.txt`.
- **SingleStore** deployment is optionally available with `docker-compose-singlestore.yml` to read all existing tables - Spark + Hive.

Attention: The code in this repository is intended for experimental use only and is not fully tested, documented, or supported by SingleStore. Visit the SingleStore Forums to ask questions about this repository.